import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron



iris = load_iris()
X = iris.data[:, (2, 3)] # petal length, petal width
y = (iris.target == 0).astype(np.int8) # Iris setosa?
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


per_clf = Perceptron()
per_clf.fit(X_train, y_train)
y_pred = per_clf.predict(X_test)


y_test[0]


from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)


from tensorflow import keras
import tensorflow as tf
fashion_mnist = keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()


X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]


class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
"Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]


model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[28, 28]))
model.add(keras.layers.Dense(600, activation="relu"))
model.add(keras.layers.Dense(400, activation="relu"))
model.add(keras.layers.Dense(10, activation="softmax"))


model = keras.models.Sequential([
keras.layers.Flatten(input_shape=[28, 28]),
keras.layers.Dense(300, activation="relu"),
keras.layers.Dense(100, activation="relu"),
keras.layers.Dense(10, activation="softmax")
])


model.summary()


model.layers


hidden1 = model.layers[1]


weights, biases = hidden1.get_weights()
print(weights)
print(biases)


print(weights.shape)
print(biases.shape)





model.compile(loss="sparse_categorical_crossentropy",
optimizer="sgd",
metrics=["accuracy"])



history = model.fit(X_train, y_train, epochs=50,
validation_data=(X_valid, y_valid))


import pandas as pd
import matplotlib.pyplot as plt
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()


model.evaluate(X_test,y_test)


x_new = X_test[:3]
y_proba = model.predict(x_new)
y_proba.round(2)


y_pred = model.predict(x_new)
y_pred


from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(
housing.data, housing.target)
X_train, X_valid, y_train, y_valid = train_test_split(
X_train_full, y_train_full)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)


model = keras.models.Sequential(
    [
        keras.layers.Dense(30,activation="relu",input_shape = X_train.shape[1:]),
        keras.layers.Dense(1)
    ]
)
model.compile(loss="mean_squared_error", optimizer="sgd")
history = model.fit(X_train,y_train,epochs = 50,validation_data = (X_valid,y_valid))



X_new = X_test[:3] # pretend these are new instances
y_pred = model.predict(X_new)


pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()


input_ = keras.layers.Input(shape = X_train.shape[1:])
hidden1 = keras.layers.Dense(30,activation = "relu")(input_)
hidden2 = keras.layers.Dense(30,activation = "relu")(hidden1)
concat  = keras.layers.Concatenate()([input_,hidden2])
output  = keras.layers.Dense(1)(concat)
model = keras.Model(inputs = [input_],outputs = [output])


input_A = keras.layers.Input(shape=[5], name="wide_input")
input_B = keras.layers.Input(shape=[6], name="deep_input")
hidden1 = keras.layers.Dense(30, activation="relu")(input_B)
hidden2 = keras.layers.Dense(30, activation="relu")(hidden1)
concat = keras.layers.concatenate([input_A, hidden2])
output = keras.layers.Dense(1, name="output")(concat)
model2 = keras.Model(inputs=[input_A, input_B], outputs=[output])


model2.compile(loss="mse")
X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]
X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]
X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]
X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]


history = model2.fit((X_train_A, X_train_B), y_train, epochs=20,
validation_data=((X_valid_A, X_valid_B), y_valid))
mse_test = model2.evaluate((X_test_A, X_test_B), y_test)
y_pred = model2.predict((X_new_A, X_new_B))


pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()


model.save("my_keras_model.h5")


model = keras.models.load_model("my_keras_model.h5")


import os
import time

# Root directory for all logs
root_logdir = os.path.join(os.curdir, "my_logs")

# Function to create a unique log directory with timestamp
def get_run_logdir():
    run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")  # Example: run_2025_07_01-14_35_00
    return os.path.join(root_logdir, run_id)

# Final path where logs will be stored for this run
run_logdir = get_run_logdir()



from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
    Dense(64, activation='relu', input_shape=X_train.shape[1:]),
    Dense(64, activation='relu'),
    Dense(1)  # Output layer for regression
])

model.compile(loss="mean_squared_error", optimizer="adam", metrics=["mae"])



tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
history = model.fit(
    X_train, y_train,
    epochs=30,
    validation_data=(X_valid, y_valid),
    callbacks=[tensorboard_cb]
)





