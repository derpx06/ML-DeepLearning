{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad8ca5f9-e4a3-407b-bfe1-65de196dce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: numpy in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (2.1.3)\n",
      "Requirement already satisfied: pandas in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (4.9.9)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: array_record>=0.5.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (0.7.2)\n",
      "Requirement already satisfied: dm-tree in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.9.1 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (1.13.0)\n",
      "Requirement already satisfied: immutabledict in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (4.2.1)\n",
      "Requirement already satisfied: promise in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: psutil in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (21.0.0)\n",
      "Requirement already satisfied: simple_parsing in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (1.17.2)\n",
      "Requirement already satisfied: toml in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-datasets) (4.67.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: einops in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (0.8.1)\n",
      "Requirement already satisfied: fsspec in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2025.7.0)\n",
      "Requirement already satisfied: importlib_resources in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (6.5.2)\n",
      "Requirement already satisfied: zipp in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (3.23.0)\n",
      "Requirement already satisfied: rich in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from dm-tree->tensorflow-datasets) (25.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from simple_parsing->tensorflow-datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /home/manas/Documents/webdev/.venv/lib/python3.12/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.70.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow numpy pandas scikit-learn tensorflow-datasets\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da68e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-19 23:26:31.553282: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1.,2.,3.],[4.,5.,6.]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799a928e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=44.0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(44, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c893a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(tf.constant([[1., 2., 3.], [4., 5., 6.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32c8f32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f81e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c509fdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t@tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7404036c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manas/Documents/webdev/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load a real-life dataset: California Housing (predict house prices)\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "\n",
    "# Split the data\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_housing, y_housing, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_h = scaler.fit_transform(X_train_h)\n",
    "X_test_h = scaler.transform(X_test_h)\n",
    "\n",
    "# Build a deep neural network for regression\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_h.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # Output: predicted house price\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6409e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "model.compile(optimizer='adam', loss=huber_fn, metrics=['mae'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88aad7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.3700 - mae: 0.6875 - val_loss: 0.1774 - val_mae: 0.4584\n",
      "Epoch 2/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1651 - mae: 0.4330 - val_loss: 0.1655 - val_mae: 0.4349\n",
      "Epoch 3/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1469 - mae: 0.4020 - val_loss: 0.1561 - val_mae: 0.4176\n",
      "Epoch 4/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1406 - mae: 0.3875 - val_loss: 0.1412 - val_mae: 0.3910\n",
      "Epoch 5/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1353 - mae: 0.3816 - val_loss: 0.1401 - val_mae: 0.3818\n",
      "Epoch 6/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1312 - mae: 0.3687 - val_loss: 0.1357 - val_mae: 0.3764\n",
      "Epoch 7/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1251 - mae: 0.3606 - val_loss: 0.1432 - val_mae: 0.3766\n",
      "Epoch 8/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1228 - mae: 0.3536 - val_loss: 0.1549 - val_mae: 0.4252\n",
      "Epoch 9/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1235 - mae: 0.3581 - val_loss: 0.1326 - val_mae: 0.3642\n",
      "Epoch 10/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1202 - mae: 0.3511 - val_loss: 0.1364 - val_mae: 0.3906\n",
      "Epoch 11/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1172 - mae: 0.3438 - val_loss: 0.1369 - val_mae: 0.3864\n",
      "Epoch 12/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1168 - mae: 0.3458 - val_loss: 0.1323 - val_mae: 0.3630\n",
      "Epoch 13/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1222 - mae: 0.3526 - val_loss: 0.1365 - val_mae: 0.3784\n",
      "Epoch 14/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1167 - mae: 0.3417 - val_loss: 0.1254 - val_mae: 0.3522\n",
      "Epoch 15/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1130 - mae: 0.3355 - val_loss: 0.1276 - val_mae: 0.3660\n",
      "Epoch 16/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1126 - mae: 0.3365 - val_loss: 0.1279 - val_mae: 0.3574\n",
      "Epoch 17/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1115 - mae: 0.3328 - val_loss: 0.1278 - val_mae: 0.3684\n",
      "Epoch 18/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1135 - mae: 0.3396 - val_loss: 0.1276 - val_mae: 0.3571\n",
      "Epoch 19/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1076 - mae: 0.3287 - val_loss: 0.1261 - val_mae: 0.3546\n",
      "Epoch 20/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1090 - mae: 0.3293 - val_loss: 0.1276 - val_mae: 0.3517\n",
      "Epoch 21/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1106 - mae: 0.3315 - val_loss: 0.1239 - val_mae: 0.3578\n",
      "Epoch 22/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1043 - mae: 0.3221 - val_loss: 0.1242 - val_mae: 0.3479\n",
      "Epoch 23/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1016 - mae: 0.3148 - val_loss: 0.1219 - val_mae: 0.3432\n",
      "Epoch 24/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1052 - mae: 0.3215 - val_loss: 0.1255 - val_mae: 0.3503\n",
      "Epoch 25/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1023 - mae: 0.3159 - val_loss: 0.1278 - val_mae: 0.3583\n",
      "Epoch 26/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1020 - mae: 0.3163 - val_loss: 0.1259 - val_mae: 0.3602\n",
      "Epoch 27/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0996 - mae: 0.3123 - val_loss: 0.1201 - val_mae: 0.3489\n",
      "Epoch 28/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0998 - mae: 0.3128 - val_loss: 0.1226 - val_mae: 0.3489\n",
      "Epoch 29/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0952 - mae: 0.3053 - val_loss: 0.1226 - val_mae: 0.3481\n",
      "Epoch 30/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0980 - mae: 0.3076 - val_loss: 0.1212 - val_mae: 0.3464\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1197 - mae: 0.3417  \n",
      "Test Hubber loss: 0.34\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted house price: $51131.56\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train_h, y_train_h, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mae = model.evaluate(X_test_h, y_test_h)\n",
    "print(f\"Test Hubber loss: {test_mae:.2f}\")\n",
    "\n",
    "# Predict house price for a new sample\n",
    "sample = X_test_h[0].reshape(1, -1)\n",
    "predicted_price = model.predict(sample)[0][0]\n",
    "print(f\"Predicted house price: ${predicted_price * 100000:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d8741a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# Save the model with a custom loss function\n",
    "model.save(\"my_model_with_a_custom_loss.h5\")\n",
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8994211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,035</span> (109.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m28,035\u001b[0m (109.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">28,033</span> (109.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m28,033\u001b[0m (109.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aca78f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(51131.562)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sample)[0][0]*100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "503283d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b842605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce89c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81fa8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48701654",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad371cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-19 23:26:53.869853: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a68a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"data.tfrecord\") as writer:\n",
    "    writer.write(\"My name is manas\")\n",
    "    writer.write(\"I study in PICT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b1c3cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'My name is manas', shape=(), dtype=string)\n",
      "tf.Tensor(b'I study in PICT', shape=(), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-19 23:26:53.905177: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:381] TFRecordDataset `buffer_size` is unspecified, default to 262144\n",
      "2025-07-19 23:26:53.905975: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data.tfrecord\"\n",
    "raw_dataset = tf.data.TFRecordDataset(file_path)\n",
    "for item in raw_dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ff1b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Compressed data example', shape=(), dtype=string)\n",
      "tf.Tensor(b'Another compressed data example', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#loading complressed files\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"Compressed data example\")\n",
    "    f.write(b\"Another compressed data example\")\n",
    "datset = tf.data.TFRecordDataset(\"my_compressed.tfrecord\", compression_type=\"GZIP\")\n",
    "for item in datset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4b7d03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.lookup_ops.StaticVocabularyTable at 0x7fda5b69bce0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7793a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 5]\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "categorical_data = tf.constant([\"INLAND\", \"NEAR OCEAN\", \"ISLAND\", \"UNKNOWN\"])\n",
    "cat_indices = table.lookup(categorical_data)\n",
    "print(cat_indices.numpy())\n",
    "cat_one_hot  = tf.one_hot(cat_indices,depth = len(vocab)+num_oov_buckets)\n",
    "print(cat_one_hot.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a02ed237",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76a553a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.5910189 , 0.22021341],\n",
       "       [0.20604026, 0.25875568],\n",
       "       [0.1538744 , 0.4310733 ],\n",
       "       [0.56827116, 0.657869  ],\n",
       "       [0.3946972 , 0.10202622],\n",
       "       [0.24898303, 0.78690934],\n",
       "       [0.6916238 , 0.6304847 ]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d511d75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.56827116 0.657869  ]\n",
      " [0.24898303 0.78690934]\n",
      " [0.20604026 0.25875568]\n",
      " [0.20604026 0.25875568]]\n"
     ]
    }
   ],
   "source": [
    "categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_embeddings = tf.nn.embedding_lookup(embedding_matrix, cat_indices)\n",
    "print(cat_embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94ddadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "embedding_dim = 2  # Make sure this matches your earlier definition\n",
    "vocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "num_oov_buckets = 2\n",
    "\n",
    "# Inputs\n",
    "regular_inputs = keras.layers.Input(shape=(8,))\n",
    "categories = keras.layers.Input(shape=(), dtype=tf.string)\n",
    "\n",
    "# Lookup indices for categories (specify output_shape)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, tf.range(len(vocab), dtype=tf.int64))\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\n",
    "cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats), output_shape=(1,), dtype=tf.int64)(categories)\n",
    "\n",
    "# Embedding for categorical features\n",
    "cat_embed = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets, output_dim=embedding_dim)(cat_indices)\n",
    "cat_embed_flat = keras.layers.Flatten()(cat_embed)\n",
    "\n",
    "# Concatenate regular and embedded categorical inputs\n",
    "encoded_inputs = keras.layers.Concatenate()([regular_inputs, cat_embed_flat])\n",
    "\n",
    "# Output layer\n",
    "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
    "\n",
    "# Model\n",
    "model = keras.models.Model(inputs=[regular_inputs, categories], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea9a75e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 4 0 0]\n",
      " [3 0 0 4 0 4 0 4]\n",
      " [0 2 4 0 4 0 4 4]\n",
      " [0 4 0 0 0 3 4 2]\n",
      " [0 3 4 0 0 0 3 0]\n",
      " [0 0 2 4 4 4 4 3]\n",
      " [4 0 3 0 1 0 4 0]\n",
      " [4 0 0 0 4 0 0 0]\n",
      " [0 0 4 0 0 4 0 4]\n",
      " [2 0 4 0 0 0 0 0]\n",
      " [0 4 2 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 4]\n",
      " [4 3 1 0 2 0 0 0]\n",
      " [0 4 0 0 4 1 0 4]\n",
      " [0 0 4 0 0 0 0 0]\n",
      " [0 0 0 0 0 3 0 2]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [4 2 4 0 1 0 0 0]\n",
      " [0 4 2 0 0 0 0 0]\n",
      " [4 0 0 3 0 0 4 0]\n",
      " [4 4 0 4 4 4 2 4]\n",
      " [0 0 0 0 4 0 2 0]\n",
      " [0 4 1 4 4 4 0 4]\n",
      " [0 0 4 0 0 4 4 4]\n",
      " [0 4 0 0 0 0 0 0]\n",
      " [4 0 0 0 0 4 4 0]\n",
      " [0 0 4 0 0 0 0 0]\n",
      " [4 0 0 4 0 0 0 4]\n",
      " [2 4 0 4 3 0 3 0]\n",
      " [0 0 0 3 1 4 4 4]\n",
      " [4 4 0 0 0 4 0 0]\n",
      " [0 0 3 2 1 0 4 0]\n",
      " [3 0 1 4 0 0 0 0]\n",
      " [4 0 3 0 0 4 4 0]\n",
      " [0 0 4 0 4 3 0 2]\n",
      " [0 0 2 2 4 0 4 0]\n",
      " [2 1 0 0 0 0 1 0]\n",
      " [4 4 0 0 0 4 4 2]\n",
      " [0 0 0 4 4 4 0 3]\n",
      " [2 3 0 0 0 2 4 0]\n",
      " [0 4 2 0 4 0 0 0]\n",
      " [0 0 3 0 0 0 0 4]\n",
      " [3 0 0 0 0 0 4 1]\n",
      " [1 4 4 4 0 0 0 0]\n",
      " [0 0 4 3 4 4 4 1]\n",
      " [0 1 4 4 0 0 0 0]\n",
      " [1 1 4 4 0 4 0 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 4 4 0 0 4 4 4]\n",
      " [0 0 0 2 0 0 0 4]\n",
      " [4 4 0 0 4 0 4 1]\n",
      " [0 2 4 4 2 3 3 4]\n",
      " [3 0 0 4 0 4 0 0]\n",
      " [4 0 0 4 0 0 0 0]\n",
      " [1 0 0 0 4 4 0 2]\n",
      " [0 0 4 0 0 0 0 0]\n",
      " [0 0 0 0 4 0 4 0]\n",
      " [0 4 4 4 0 4 0 4]\n",
      " [0 0 0 4 4 4 4 2]\n",
      " [3 0 0 4 2 0 0 3]\n",
      " [0 0 0 0 4 4 2 0]\n",
      " [4 1 4 4 0 3 0 0]\n",
      " [4 1 4 0 0 0 0 1]\n",
      " [4 0 1 0 4 0 4 0]\n",
      " [1 0 0 3 0 0 0 4]\n",
      " [0 0 4 4 4 0 4 1]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [2 0 0 4 0 0 1 4]\n",
      " [0 2 2 4 2 3 3 4]\n",
      " [0 0 0 0 3 0 0 0]\n",
      " [4 1 0 4 0 4 4 4]\n",
      " [0 0 0 0 4 0 0 0]\n",
      " [1 4 4 0 0 0 0 4]\n",
      " [3 0 0 3 0 0 4 4]\n",
      " [0 4 0 3 4 0 0 0]\n",
      " [2 4 0 4 4 4 4 3]\n",
      " [0 4 0 0 4 0 2 3]\n",
      " [4 4 0 0 0 3 0 0]\n",
      " [0 4 0 2 0 4 0 4]\n",
      " [0 4 0 0 1 0 0 1]\n",
      " [2 4 4 0 0 4 0 0]\n",
      " [0 0 1 0 0 4 0 4]\n",
      " [4 0 1 0 4 0 3 0]\n",
      " [0 2 4 0 0 4 0 0]\n",
      " [0 0 0 0 1 0 0 1]\n",
      " [4 4 0 0 4 0 0 0]\n",
      " [2 4 0 4 4 0 0 0]\n",
      " [4 0 4 0 0 0 4 0]\n",
      " [4 0 0 1 2 0 0 3]\n",
      " [0 2 0 0 3 2 3 0]\n",
      " [3 0 4 4 0 4 4 2]\n",
      " [0 0 0 4 1 0 4 2]\n",
      " [0 0 4 3 0 0 0 0]\n",
      " [4 0 0 1 0 0 0 4]\n",
      " [0 0 4 4 0 3 0 4]\n",
      " [4 0 0 4 0 0 4 3]\n",
      " [3 1 0 0 0 0 2 4]\n",
      " [4 3 0 0 0 0 4 0]\n",
      " [0 0 2 0 4 0 0 4]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "X_train_h = np.random.rand(100, 8)  # 100 samples, 8 features\n",
    "\n",
    "# Normalization layer\n",
    "normalization = keras.layers.Normalization()\n",
    "\n",
    "# Discretization layer (example bin boundaries, replace [...] with actual values)\n",
    "bin_boundaries = [0.2, 0.4, 0.6, 0.8]  # Example boundaries for demonstration\n",
    "discretization = keras.layers.Discretization(bin_boundaries)\n",
    "\n",
    "# Build preprocessing pipeline\n",
    "pipeline = keras.Sequential([normalization, discretization])\n",
    "\n",
    "# Adapt normalization layer to your data\n",
    "normalization.adapt(X_train_h)\n",
    "\n",
    "# Adapt discretization layer to your data (optional, only needed if you want to use adapt)\n",
    "# discretization.adapt(X_train_h)  # Uncomment if you want to adapt discretization\n",
    "\n",
    "# Example: Transform data using the pipeline\n",
    "X_transformed = pipeline(X_train_h)\n",
    "print(X_transformed.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f87b8411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60000, 28, 28) (60000,)\n",
      "Test shape: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Download MNIST directly from Keras (no tfds needed)\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "mnist_train = (X_train, y_train)\n",
    "mnist_test = (X_test, y_test)\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb9dd17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-19 23:27:00.690826: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "dataset = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c0c77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(10000).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "174b120f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manas/Documents/webdev/.venv/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# Normalize and expand dims for channel\n",
    "X_train = X_train.astype(\"float32\") / 255.0\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "X_train = X_train[..., tf.newaxis]\n",
    "X_test = X_test[..., tf.newaxis]\n",
    "\n",
    "mnist_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(10000).batch(32).prefetch(1)\n",
    "mnist_test = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32).prefetch(1)\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, 3, activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13ffefa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 34ms/step - accuracy: 0.9164 - loss: 0.2833 - val_accuracy: 0.9778 - val_loss: 0.0727\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 32ms/step - accuracy: 0.9745 - loss: 0.0850 - val_accuracy: 0.9880 - val_loss: 0.0351\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 32ms/step - accuracy: 0.9817 - loss: 0.0608 - val_accuracy: 0.9849 - val_loss: 0.0415\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 30ms/step - accuracy: 0.9843 - loss: 0.0509 - val_accuracy: 0.9896 - val_loss: 0.0321\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 30ms/step - accuracy: 0.9875 - loss: 0.0380 - val_accuracy: 0.9892 - val_loss: 0.0368\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 30ms/step - accuracy: 0.9890 - loss: 0.0347 - val_accuracy: 0.9911 - val_loss: 0.0304\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 23ms/step - accuracy: 0.9902 - loss: 0.0321 - val_accuracy: 0.9919 - val_loss: 0.0280\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 35ms/step - accuracy: 0.9909 - loss: 0.0286 - val_accuracy: 0.9919 - val_loss: 0.0296\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 36ms/step - accuracy: 0.9917 - loss: 0.0250 - val_accuracy: 0.9917 - val_loss: 0.0258\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 35ms/step - accuracy: 0.9931 - loss: 0.0221 - val_accuracy: 0.9920 - val_loss: 0.0278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fda5b5201d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mnist_train, epochs=10, validation_data=mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90e7fe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9900 - loss: 0.0336\n",
      "Test accuracy: 0.9919999837875366\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(mnist_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "813b7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "def load_image(name):\n",
    "    \"\"\"\n",
    "    Loads a sample image from sklearn and normalizes it to [0, 1].\n",
    "    Args:\n",
    "        name (str): The filename, e.g., \"china.jpg\" or \"flower.jpg\"\n",
    "    Returns:\n",
    "        np.ndarray: Normalized image array\n",
    "    \"\"\"\n",
    "    img = load_sample_image(name)\n",
    "    return img / 255.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdf1b5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "The Python Imaging Library (PIL) is required to load data from jpeg files. Please refer to https://pillow.readthedocs.io/en/stable/installation.html for installing PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/webdev/.venv/lib/python3.12/site-packages/sklearn/datasets/_base.py:1327\u001b[39m, in \u001b[36mload_sample_images\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'PIL'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_sample_image\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load sample images\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m china = \u001b[43mload_sample_image\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchina.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m / \u001b[32m255\u001b[39m\n\u001b[32m      5\u001b[39m flower = load_sample_image(\u001b[33m\"\u001b[39m\u001b[33mflower.jpg\u001b[39m\u001b[33m\"\u001b[39m) / \u001b[32m255\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/webdev/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/webdev/.venv/lib/python3.12/site-packages/sklearn/datasets/_base.py:1392\u001b[39m, in \u001b[36mload_sample_image\u001b[39m\u001b[34m(image_name)\u001b[39m\n\u001b[32m   1356\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m   1357\u001b[39m     {\n\u001b[32m   1358\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage_name\u001b[39m\u001b[33m\"\u001b[39m: [StrOptions({\u001b[33m\"\u001b[39m\u001b[33mchina.jpg\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mflower.jpg\u001b[39m\u001b[33m\"\u001b[39m})],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1361\u001b[39m )\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_sample_image\u001b[39m(image_name):\n\u001b[32m   1363\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load the numpy array of a single sample image.\u001b[39;00m\n\u001b[32m   1364\u001b[39m \n\u001b[32m   1365\u001b[39m \u001b[33;03m    Read more in the :ref:`User Guide <sample_images>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1390\u001b[39m \u001b[33;03m    (427, 640, 3)\u001b[39;00m\n\u001b[32m   1391\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     images = \u001b[43mload_sample_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m     index = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, filename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(images.filenames):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/webdev/.venv/lib/python3.12/site-packages/sklearn/datasets/_base.py:1329\u001b[39m, in \u001b[36mload_sample_images\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   1330\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe Python Imaging Library (PIL) is required to load data \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfrom jpeg files. Please refer to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pillow.readthedocs.io/en/stable/installation.html \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1333\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor installing PIL.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1336\u001b[39m descr = load_descr(\u001b[33m\"\u001b[39m\u001b[33mREADME.txt\u001b[39m\u001b[33m\"\u001b[39m, descr_module=IMAGES_MODULE)\n\u001b[32m   1338\u001b[39m filenames, images = [], []\n",
      "\u001b[31mImportError\u001b[39m: The Python Imaging Library (PIL) is required to load data from jpeg files. Please refer to https://pillow.readthedocs.io/en/stable/installation.html for installing PIL."
     ]
    }
   ],
   "source": [
    "!pip install PIL\n",
    "from sklearn.datasets import load_sample_image\n",
    "# Load sample images\n",
    "china = load_sample_image(\"china.jpg\") / 255\n",
    "flower = load_sample_image(\"flower.jpg\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a37203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
